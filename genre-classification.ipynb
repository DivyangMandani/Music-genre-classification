{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, applications\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Function to extract spectrogram from audio file and convert it to an image-like format\ndef extract_spectrogram(audio_path, target_size=(224, 224)):\n    try:\n        audio, sr = librosa.load(audio_path, sr=None)\n        spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n        # Convert the spectrogram to dB scale\n        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n        # Resize the image to the target size\n        spectrogram_image_resized = tf.image.resize(np.expand_dims(spectrogram_db, axis=-1), target_size)\n        return spectrogram_image_resized.numpy()\n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n        return None\n\n# Path to the folder containing audio files\naudio_folder_path = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\"\n\n# List of category labels\ncategory_labels = os.listdir(audio_folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:01:12.110727Z","iopub.execute_input":"2024-01-05T10:01:12.111832Z","iopub.status.idle":"2024-01-05T10:01:12.127794Z","shell.execute_reply.started":"2024-01-05T10:01:12.111789Z","shell.execute_reply":"2024-01-05T10:01:12.126691Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"category_labels","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:01:12.129829Z","iopub.execute_input":"2024-01-05T10:01:12.130176Z","iopub.status.idle":"2024-01-05T10:01:12.146394Z","shell.execute_reply.started":"2024-01-05T10:01:12.130146Z","shell.execute_reply":"2024-01-05T10:01:12.145346Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"['disco',\n 'metal',\n 'reggae',\n 'blues',\n 'rock',\n 'classical',\n 'jazz',\n 'hiphop',\n 'country',\n 'pop']"},"metadata":{}}]},{"cell_type":"code","source":"# Extract spectrograms for each audio file using ResNet-50\nfeatures_list = []\nlabels_list = []\nfor label in category_labels:\n    label_folder = os.path.join(audio_folder_path, label)\n    for audio_file in os.listdir(label_folder):\n        audio_path = os.path.join(label_folder, audio_file)\n        features = extract_spectrogram(audio_path, target_size=(224, 224))\n        if features is not None:\n            # Duplicate the single channel to create three channels\n            features_rgb = np.concatenate([features, features, features], axis=-1)\n            features_list.append(features_rgb)\n            labels_list.append(category_labels.index(label))\n\n# Convert lists to numpy arrays\nfeatures_array = np.array(features_list)\nlabels_array = np.array(labels_list)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(\n    features_array, labels_array, test_size=0.2, random_state=42)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T10:01:12.148152Z","iopub.execute_input":"2024-01-05T10:01:12.148534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build and compile the ResNet-50 model\nbase_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Choose the number of layers to unfreeze\nnum_layers_to_unfreeze = 40\n\n# Unfreeze layers from the top\nfor layer in base_model.layers[:-num_layers_to_unfreeze]:\n    layer.trainable = False\n\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(len(category_labels), activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n# Train model\nhistory = model.fit(\n    X_train,\n    y_train,\n    epochs=100,\n    validation_data=(X_val, y_val),\n    verbose=0\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model on the test set\npredictions_prob = model.predict(X_test)\npredictions = np.argmax(predictions_prob, axis=1)\n\n# Calculate accuracy and display results\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy of the model:\", accuracy)\n\n# Plot training history\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Training Set\", \"Validation Set\"])\n\n# Confusion Matrix\nfig, ax = plt.subplots(figsize=(12, 8))\nconf_matrix = confusion_matrix(y_test, predictions)\nConfusionMatrixDisplay(conf_matrix, display_labels=category_labels).plot(ax=ax)\n\n# Classification Report\nreport = classification_report(y_test, predictions)\nprint('\\nClassification Report:\\n', report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('dl_model2.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}